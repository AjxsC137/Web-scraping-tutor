{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check if the Jira API is accessible"
      ],
      "metadata": {
        "id": "cjMyC3xufLXv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNGbqKt69jOs",
        "outputId": "83a8079a-0c57-41eb-d091-602427c844f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jira API is accessible. Status code: 200\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Common Jira API\n",
        "jira_api_url = \"https://issues.apache.org/jira/rest/api/2/project\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(jira_api_url)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "    print(\"Jira API is accessible. Status code:\", response.status_code)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error accessing Jira API: {e}\")\n",
        "    print(\"It seems a public API might not be directly accessible or requires authentication.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching and displaying information about available projects"
      ],
      "metadata": {
        "id": "Rkxo2swGfTKy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "jira_api_url = \"https://issues.apache.org/jira/rest/api/2/project\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(jira_api_url)\n",
        "    response.raise_for_status()\n",
        "    projects = response.json()\n",
        "\n",
        "    print(f\"Found {len(projects)} projects.\")\n",
        "    if projects:\n",
        "        print(\"\\nKeys for the first project:\")\n",
        "        print(projects[0].keys())\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error accessing Jira API: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBmEZ0uJBUO7",
        "outputId": "2354f64e-7131-4da7-cbda-0108c24424a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 672 projects.\n",
            "\n",
            "Keys for the first project:\n",
            "dict_keys(['expand', 'self', 'id', 'key', 'name', 'avatarUrls', 'projectTypeKey', 'archived'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search and select Spark, Hadoop, and Kafka related projects"
      ],
      "metadata": {
        "id": "Kn6gWvK2fpQR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "jira_api_url = \"https://issues.apache.org/jira/rest/api/2/project\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(jira_api_url)\n",
        "    response.raise_for_status()\n",
        "    projects = response.json()\n",
        "\n",
        "    spark_projects = [p for p in projects if 'spark' in p['key'].lower() or 'spark' in p['name'].lower()]\n",
        "    hadoop_projects = [p for p in projects if 'hadoop' in p['key'].lower() or 'hadoop' in p['name'].lower()]\n",
        "    kafka_projects = [p for p in projects if 'kafka' in p['key'].lower() or 'kafka' in p['name'].lower()]\n",
        "\n",
        "    print(f\"Found {len(spark_projects)} Spark related projects.\")\n",
        "    print(f\"Found {len(hadoop_projects)} Hadoop related projects.\")\n",
        "    print(f\"Found {len(kafka_projects)} Kafka related projects.\")\n",
        "\n",
        "    selected_projects = []\n",
        "    if spark_projects:\n",
        "        selected_projects.append(spark_projects[0])\n",
        "        print(f\"\\nSelected Spark project: {spark_projects[0]['key']}\")\n",
        "    if hadoop_projects:\n",
        "        selected_projects.append(hadoop_projects[0])\n",
        "        print(f\"Selected Hadoop project: {hadoop_projects[0]['key']}\")\n",
        "    if kafka_projects:\n",
        "        selected_projects.append(kafka_projects[0])\n",
        "        print(f\"Selected Kafka project: {kafka_projects[0]['key']}\")\n",
        "\n",
        "    if selected_projects:\n",
        "        print(\"\\nSelected projects for scraping:\")\n",
        "        for project in selected_projects:\n",
        "            print(f\"- {project['key']}: {project['name']}\")\n",
        "    else:\n",
        "        print(\"\\nCould not find projects matching the criteria.\")\n",
        "\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error accessing Jira API: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFAi0qKZCaLb",
        "outputId": "a4c98fdc-23ae-4b1e-ee8f-dd8e9065c865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 Spark related projects.\n",
            "Found 5 Hadoop related projects.\n",
            "Found 1 Kafka related projects.\n",
            "\n",
            "Selected Spark project: SPARK\n",
            "Selected Hadoop project: HADOOP\n",
            "Selected Kafka project: KAFKA\n",
            "\n",
            "Selected projects for scraping:\n",
            "- SPARK: Spark\n",
            "- HADOOP: Hadoop Common\n",
            "- KAFKA: Kafka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING TO SEE IF WE CAN FETCH A SINGLE ISSUE"
      ],
      "metadata": {
        "id": "urgOhmSWDkde"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "issue_key = \"KAFKA-1\" # Placeholder key\n",
        "\n",
        "jira_api_url = f\"https://issues.apache.org/jira/rest/api/2/issue/{issue_key}\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(jira_api_url)\n",
        "    response.raise_for_status()\n",
        "    issue_data = response.json()\n",
        "\n",
        "    print(f\"Successfully fetched data for issue: {issue_key}\")\n",
        "    print(f\"Summary: {issue_data['fields']['summary']}\")\n",
        "    print(f\"Status: {issue_data['fields']['status']['name']}\")\n",
        "    print(f\"Created: {issue_data['fields']['created']}\")\n",
        "    print(f\"Updated: {issue_data['fields']['updated']}\")\n",
        "    print(f\"Comments count: {issue_data['fields']['comment']['total']}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data for issue {issue_key}: {e}\")\n",
        "    print(\"Please ensure the issue key is valid and exists in the project.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDndjA2LC4ty",
        "outputId": "b2bd3379-85a5-4c90-a6d8-a08dca7619e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched data for issue: KAFKA-2\n",
            "Summary: a restful producer API\n",
            "Status: Resolved\n",
            "Created: 2011-07-19T21:32:09.586+0000\n",
            "Updated: 2016-08-26T22:35:11.849+0000\n",
            "Comments count: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping issues with pagination, error handling, retries, and resume functionality"
      ],
      "metadata": {
        "id": "RMPZj2RygY1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "jira_search_url = \"https://issues.apache.org/jira/rest/api/2/search\"\n",
        "\n",
        "jql_query = \"project=KAFKA ORDER BY key ASC\"\n",
        "max_results = 500\n",
        "\n",
        "output_filename = \"kafka_issues_raw.json\"\n",
        "progress_filename = \"kafka_scrape_progress.json\"\n",
        "\n",
        "all_issues = []\n",
        "start_at = 0\n",
        "\n",
        "#Resume functionality\n",
        "if os.path.exists(progress_filename):\n",
        "    try:\n",
        "        with open(progress_filename, 'r') as f:\n",
        "            progress_data = json.load(f)\n",
        "            start_at = progress_data.get('last_start_at', 0)\n",
        "            print(f\"Resuming scraping from startAt={start_at}\")\n",
        "\n",
        "        if os.path.exists(output_filename):\n",
        "             try:\n",
        "                 with open(output_filename, 'r', encoding='utf-8') as f:\n",
        "                     all_issues = json.load(f)\n",
        "                 print(f\"Loaded {len(all_issues)} issues from previous run.\")\n",
        "             except json.JSONDecodeError:\n",
        "                 print(f\"Warning: Could not decode existing {output_filename}. Starting fresh.\")\n",
        "                 all_issues = []\n",
        "                 start_at = 0 # Reset if file is corrupt\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Warning: Could not decode {progress_filename}. Starting fresh.\")\n",
        "        start_at = 0\n",
        "\n",
        "# Retry parameters\n",
        "max_retries = 5\n",
        "retry_delay = 5\n",
        "\n",
        "print(f\"Fetching issues for JQL query: {jql_query}\")\n",
        "\n",
        "while True:\n",
        "    params = {\n",
        "        'jql': jql_query,\n",
        "        'startAt': start_at,\n",
        "        'maxResults': max_results\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(jira_search_url, params=params)\n",
        "            response.raise_for_status()\n",
        "            search_results = response.json()\n",
        "\n",
        "            issues = search_results.get('issues', [])\n",
        "            total_issues = search_results.get('total', 0)\n",
        "\n",
        "            all_issues.extend(issues)\n",
        "\n",
        "            print(f\"Fetched {len(issues)} issues from startAt={start_at}. Total issues found so far: {len(all_issues)}\")\n",
        "\n",
        "            #Save progress and data periodically\n",
        "            if len(issues) > 0 or start_at == 0:\n",
        "                try:\n",
        "                    # Save scraped data\n",
        "                    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(all_issues, f, indent=4)\n",
        "\n",
        "                    # Save progress\n",
        "                    progress_data = {'last_start_at': start_at + len(issues)}\n",
        "                    with open(progress_filename, 'w') as f:\n",
        "                        json.dump(progress_data, f)\n",
        "\n",
        "                except IOError as e:\n",
        "                    print(f\"Warning: Could not save progress or data: {e}\")\n",
        "\n",
        "            if (start_at + max_results) >= total_issues:\n",
        "                print(\"\\nFinished fetching all issues.\")\n",
        "                break\n",
        "            else:\n",
        "                start_at += max_results\n",
        "                time.sleep(1)\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} of {max_retries} failed: {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(f\"Max retries reached for batch starting at {start_at}. Skipping this batch.\")\n",
        "                total_issues = 0\n",
        "                break\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON response.\")\n",
        "            print(f\"Skipping batch starting at {start_at} due to JSON error.\")\n",
        "            break\n",
        "\n",
        "    if (start_at + max_results) >= total_issues and total_issues > 0:\n",
        "        break\n",
        "\n",
        "print(f\"\\nFinished fetching. Total issues collected: {len(all_issues)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Un7VwaHP5z",
        "outputId": "1fff60b4-1c32-4163-b828-c0d2052a644c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming scraping from startAt=18500\n",
            "Loaded 18500 issues from previous run.\n",
            "Fetching issues for JQL query: project=KAFKA ORDER BY key ASC\n",
            "Fetched 84 issues from startAt=18500. Total issues found so far: 18584\n",
            "\n",
            "Finished fetching all issues.\n",
            "\n",
            "Finished fetching. Total issues collected: 18584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_filename = \"kafka_issues_raw.json\"\n",
        "try:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_issues, f, indent=4)\n",
        "\n",
        "    print(f\"Successfully saved scraped data to {output_filename}\")\n",
        "\n",
        "except IOError as e:\n",
        "    print(f\"Error saving data to file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrq55e7DHAUM",
        "outputId": "464d9c79-db44-4e44-dbe8-51172cd19c23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved scraped data to kafka_issues_raw.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86c9dce",
        "outputId": "6b8eff68-1e08-42c0-c777-bbd2bf605da6"
      },
      "source": [
        "import json\n",
        "\n",
        "raw_data_path = \"/content/kafka_issues_raw.json\"\n",
        "try:\n",
        "    with open(raw_data_path, 'r', encoding='utf-8') as f:\n",
        "        kafka_issues_raw = json.load(f)\n",
        "\n",
        "    print(f\"Successfully loaded raw data from {raw_data_path}\")\n",
        "    print(f\"Number of issues loaded: {len(kafka_issues_raw)}\")\n",
        "\n",
        "    spark_issues_raw = kafka_issues_raw\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {raw_data_path} was not found.\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from {raw_data_path}. The file might be corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded raw data from /content/kafka_issues_raw.json\n",
            "Number of issues loaded: 18584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1233a290",
        "outputId": "2fca53df-22c5-4669-f937-6f8cdf2bef57"
      },
      "source": [
        "import json\n",
        "\n",
        "if 'spark_issues_raw' in locals() and len(spark_issues_raw) > 0:\n",
        "    sample_issue = spark_issues_raw[0]\n",
        "\n",
        "    print(\"Keys of a sample issue:\")\n",
        "    print(sample_issue.keys())\n",
        "\n",
        "    if 'fields' in sample_issue:\n",
        "        print(\"\\nKeys within the 'fields' section:\")\n",
        "        print(sample_issue['fields'].keys())\n",
        "\n",
        "        print(\"\\nStructure of key fields:\")\n",
        "        if 'summary' in sample_issue['fields']:\n",
        "            print(f\"- Summary: {type(sample_issue['fields']['summary'])}\")\n",
        "        if 'description' in sample_issue['fields']:\n",
        "            print(f\"- Description: {type(sample_issue['fields']['description'])}\")\n",
        "        if 'comment' in sample_issue['fields'] and 'comments' in sample_issue['fields']['comment']:\n",
        "             print(f\"- Comments: {type(sample_issue['fields']['comment']['comments'])} containing {len(sample_issue['fields']['comment']['comments'])} comments (if any)\")\n",
        "             if len(sample_issue['fields']['comment']['comments']) > 0:\n",
        "                 print(f\"  - Structure of a sample comment: {type(sample_issue['fields']['comment']['comments'][0])}\")\n",
        "                 print(f\"  - Keys of a sample comment: {sample_issue['fields']['comment']['comments'][0].keys()}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Raw data not loaded or empty. Please run the previous cell to load the data.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of a sample issue:\n",
            "dict_keys(['expand', 'id', 'self', 'key', 'fields'])\n",
            "\n",
            "Keys within the 'fields' section:\n",
            "dict_keys(['fixVersions', 'resolution', 'customfield_12312322', 'customfield_12312323', 'customfield_12310420', 'customfield_12312320', 'customfield_12312321', 'customfield_12312328', 'customfield_12312329', 'customfield_12312326', 'customfield_12312327', 'customfield_12310300', 'customfield_12312324', 'customfield_12312720', 'customfield_12312325', 'lastViewed', 'priority', 'labels', 'customfield_12312333', 'customfield_12312334', 'customfield_12313422', 'customfield_12312331', 'customfield_12310310', 'customfield_12312332', 'aggregatetimeoriginalestimate', 'timeestimate', 'customfield_12312330', 'versions', 'customfield_12311120', 'customfield_12313826', 'issuelinks', 'customfield_12312339', 'customfield_12313825', 'assignee', 'customfield_12312337', 'customfield_12313823', 'customfield_12312338', 'customfield_12311920', 'customfield_12313822', 'customfield_12312335', 'customfield_12313821', 'customfield_12312336', 'customfield_12313820', 'status', 'components', 'customfield_12312026', 'customfield_12312023', 'customfield_12312024', 'aggregatetimeestimate', 'customfield_12312022', 'customfield_12310921', 'customfield_12310920', 'customfield_12312823', 'creator', 'subtasks', 'reporter', 'aggregateprogress', 'customfield_12313520', 'customfield_12310250', 'progress', 'customfield_12313924', 'votes', 'customfield_12311620', 'customfield_12313920', 'issuetype', 'timespent', 'customfield_12314020', 'customfield_12314141', 'customfield_12314140', 'project', 'aggregatetimespent', 'customfield_12312520', 'customfield_12312521', 'customfield_12314422', 'customfield_12314421', 'customfield_12314146', 'customfield_12314420', 'customfield_12314145', 'customfield_12314144', 'customfield_12314143', 'resolutiondate', 'workratio', 'customfield_12312923', 'customfield_12312920', 'customfield_12312921', 'watches', 'created', 'updated', 'timeoriginalestimate', 'description', 'customfield_10010', 'customfield_12314523', 'customfield_12314127', 'customfield_12314522', 'customfield_12314126', 'customfield_12314521', 'customfield_12314125', 'customfield_12310320', 'customfield_12314520', 'customfield_12314124', 'customfield_12312340', 'customfield_12314123', 'customfield_12312341', 'customfield_12312220', 'customfield_12314122', 'customfield_12314121', 'customfield_12314120', 'customfield_12314129', 'customfield_12314524', 'customfield_12314128', 'summary', 'customfield_12314130', 'customfield_12310291', 'customfield_12310290', 'customfield_12311024', 'customfield_12314138', 'customfield_12314137', 'environment', 'customfield_12314136', 'customfield_12314135', 'customfield_12311020', 'customfield_12314134', 'duedate', 'customfield_12314132', 'customfield_12314131', 'customfield_12311820', 'customfield_12314139'])\n",
            "\n",
            "Structure of key fields:\n",
            "- Summary: <class 'str'>\n",
            "- Description: <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract, clean, and derive tasks from issue data"
      ],
      "metadata": {
        "id": "uyg9znJAh9Rs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "transformed_issues = []\n",
        "\n",
        "if 'spark_issues_raw' in locals() and len(spark_issues_raw) > 0:\n",
        "    print(f\"Starting extraction, cleaning, and task derivation for {len(spark_issues_raw)} issues.\")\n",
        "\n",
        "    for issue in spark_issues_raw:\n",
        "        issue_data = {}\n",
        "        issue_data['key'] = issue.get('key')\n",
        "\n",
        "        fields = issue.get('fields', {})\n",
        "\n",
        "        issue_data['summary'] = fields.get('summary')\n",
        "        issue_data['description'] = fields.get('description')\n",
        "\n",
        "        #status\n",
        "        status = fields.get('status', {})\n",
        "        issue_data['status'] = status.get('name') if status else None\n",
        "\n",
        "        #reporter\n",
        "        reporter = fields.get('reporter', {})\n",
        "        issue_data['reporter'] = reporter.get('displayName') if reporter else None\n",
        "\n",
        "        #dates\n",
        "        issue_data['created'] = fields.get('created')\n",
        "        issue_data['updated'] = fields.get('updated')\n",
        "\n",
        "        if issue_data['description']:\n",
        "            issue_data['description'] = re.sub(r'<.*?>', '', issue_data['description'])\n",
        "            issue_data['description'] = issue_data['description'].strip()\n",
        "\n",
        "        if issue_data['summary']:\n",
        "             issue_data['summary'] = re.sub(r'<.*?>', '', issue_data['summary'])\n",
        "             issue_data['summary'] = issue_data['summary'].strip()\n",
        "\n",
        "        #Process comments\n",
        "        comments_data = fields.get('comment', {})\n",
        "        comment_list = comments_data.get('comments', [])\n",
        "        issue_data['comments'] = []\n",
        "        for comment in comment_list:\n",
        "            comment_body = comment.get('body')\n",
        "            if comment_body:\n",
        "                cleaned_comment = re.sub(r'<.*?>', '', comment_body)\n",
        "                cleaned_comment = cleaned_comment.strip()\n",
        "                issue_data['comments'].append(cleaned_comment)\n",
        "\n",
        "        #Derive Tasks\n",
        "        issue_data['summarization_task'] = {\n",
        "            \"instruction\": \"Summarize the following Jira issue:\",\n",
        "            \"input\": f\"Summary: {issue_data.get('summary', '')}\\nDescription: {issue_data.get('description', '')}\\nComments: {' '.join(issue_data.get('comments', []))}\",\n",
        "            \"output\": \"Generated summary goes here.\"\n",
        "        }\n",
        "\n",
        "        #Classification Task\n",
        "        issue_data['classification_task'] = {\n",
        "            \"instruction\": \"Classify the type of the following Jira issue:\",\n",
        "            \"input\": f\"Summary: {issue_data.get('summary', '')}\\nDescription: {issue_data.get('description', '')}\",\n",
        "            \"output\": issue_data.get('status', 'Unknown')\n",
        "        }\n",
        "\n",
        "        #Question Answering Task\n",
        "        issue_data['qna_task'] = {\n",
        "            \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
        "            \"input\": f\"Issue: Summary: {issue_data.get('summary', '')}\\nDescription: {issue_data.get('description', '')}\\nComments: {' '.join(issue_data.get('comments', []))}\\nQuestion: What is the main problem described in this issue?\", # Replace with actual generated question\n",
        "            \"output\": issue_data.get('summary', 'No summary provided.')\n",
        "        }\n",
        "\n",
        "\n",
        "        transformed_issues.append(issue_data)\n",
        "\n",
        "    print(f\"Finished extraction, cleaning, and task derivation. Created {len(transformed_issues)} transformed issue entries.\")\n",
        "\n",
        "else:\n",
        "    print(\"Raw data not loaded or empty. Please run the loading cell first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLGIdkV1UjhB",
        "outputId": "aca8f8ae-57ef-48ad-e3c1-c3655462f171"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting extraction, cleaning, and task derivation for 18584 issues.\n",
            "Finished extraction, cleaning, and task derivation. Created 18584 transformed issue entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "516a2842",
        "outputId": "03e2d636-3b48-43a3-ae76-32c1cd5e8084"
      },
      "source": [
        "import json\n",
        "\n",
        "if 'transformed_issues' in locals() and transformed_issues:\n",
        "    print(\"Sample of transformed issue data:\")\n",
        "    print(json.dumps(transformed_issues[0], indent=4))\n",
        "else:\n",
        "    print(\"Transformed data not available. Please run the previous transformation steps.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of transformed issue data:\n",
            "{\n",
            "    \"key\": \"SPARK-290\",\n",
            "    \"summary\": \"Use SPARK_MASTER_IP if it is set in start-slaves.sh.\",\n",
            "    \"description\": \"Also check to prompt user if it is not set and the script cannot figure out the master's ip.\",\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Reynold Xin\",\n",
            "    \"created\": \"0012-10-19T00:09:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:34.000+0000\",\n",
            "    \"comments\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1960426a",
        "outputId": "48dc3d01-1932-4e65-c0b2-367690c5e2ee"
      },
      "source": [
        "import json\n",
        "\n",
        "if 'transformed_issues' in locals() and transformed_issues:\n",
        "    output_jsonl_filename = \"kafka_issues_transformed.jsonl\"\n",
        "\n",
        "    try:\n",
        "        with open(output_jsonl_filename, 'w', encoding='utf-8') as f:\n",
        "            for issue in transformed_issues:\n",
        "                #each issue as a JSON object on a new line\n",
        "                f.write(json.dumps(issue, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"Successfully saved transformed data to {output_jsonl_filename}\")\n",
        "\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving transformed data to file: {e}\")\n",
        "else:\n",
        "    print(\"Transformed data not available. Please run the transformation steps first.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved transformed data to kafka_issues_transformed.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae438165",
        "outputId": "2faafad2-2b9e-415f-df2c-a73109dbeeb4"
      },
      "source": [
        "import os\n",
        "\n",
        "spark_file = \"spark_issues_transformed.jsonl\"\n",
        "hadoop_file = \"hadoop_issues_transformed.jsonl\"\n",
        "kafka_file = \"kafka_issues_transformed.jsonl\"\n",
        "\n",
        "files = [spark_file, hadoop_file, kafka_file]\n",
        "\n",
        "print(\"File sizes of transformed data:\")\n",
        "\n",
        "for file in files:\n",
        "    if os.path.exists(file):\n",
        "        size_in_bytes = os.path.getsize(file)\n",
        "        size_in_kb = size_in_bytes / 1024\n",
        "        size_in_mb = size_in_kb / 1024\n",
        "        print(f\"- {file}: {size_in_bytes} bytes ({size_in_kb:.2f} KB, {size_in_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"- {file}: Not found\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File sizes of transformed data:\n",
            "- spark_issues_transformed.jsonl: 284047402 bytes (277390.04 KB, 270.89 MB)\n",
            "- hadoop_issues_transformed.jsonl: 284047402 bytes (277390.04 KB, 270.89 MB)\n",
            "- kafka_issues_transformed.jsonl: 284047402 bytes (277390.04 KB, 270.89 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1996a4cc",
        "outputId": "16820c7a-33d0-42a0-a9d9-1d3ebf477dad"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "output_combined_filename = \"all_issues_transformed.jsonl\"\n",
        "input_files = [\n",
        "    \"spark_issues_transformed.jsonl\",\n",
        "    \"hadoop_issues_transformed.jsonl\",\n",
        "    \"kafka_issues_transformed.jsonl\"\n",
        "]\n",
        "\n",
        "print(f\"Starting to combine transformed data into {output_combined_filename}\")\n",
        "\n",
        "try:\n",
        "    with open(output_combined_filename, 'w', encoding='utf-8') as outfile:\n",
        "        for input_file in input_files:\n",
        "            if os.path.exists(input_file):\n",
        "                print(f\"Reading from {input_file}...\")\n",
        "                with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "                    for line in infile:\n",
        "                        outfile.write(line)\n",
        "            else:\n",
        "                print(f\"Warning: Input file not found: {input_file}. Skipping.\")\n",
        "\n",
        "    print(f\"Successfully combined data into {output_combined_filename}\")\n",
        "\n",
        "except IOError as e:\n",
        "    print(f\"Error combining data: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to combine transformed data into all_issues_transformed.jsonl\n",
            "Reading from spark_issues_transformed.jsonl...\n",
            "Reading from hadoop_issues_transformed.jsonl...\n",
            "Reading from kafka_issues_transformed.jsonl...\n",
            "Successfully combined data into all_issues_transformed.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88621444",
        "outputId": "9e494567-7bf2-486e-87c7-9a65ea96dadf"
      },
      "source": [
        "import os\n",
        "\n",
        "combined_file = \"all_issues_transformed.jsonl\"\n",
        "\n",
        "print(f\"Checking the size of the combined file: {combined_file}\")\n",
        "\n",
        "if os.path.exists(combined_file):\n",
        "    size_in_bytes = os.path.getsize(combined_file)\n",
        "    size_in_kb = size_in_bytes / 1024\n",
        "    size_in_mb = size_in_kb / 1024\n",
        "    print(f\"- {combined_file}: {size_in_bytes} bytes ({size_in_kb:.2f} KB, {size_in_mb:.2f} MB)\")\n",
        "else:\n",
        "    print(f\"- {combined_file}: Not found\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the size of the combined file: all_issues_transformed.jsonl\n",
            "- all_issues_transformed.jsonl: 852142206 bytes (832170.12 KB, 812.67 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5286ebe9",
        "outputId": "eb5207ee-f0c1-4268-9484-2fc88f61dd6c"
      },
      "source": [
        "import json\n",
        "\n",
        "combined_file = \"all_issues_transformed.jsonl\"\n",
        "\n",
        "print(f\"\\nDisplaying sample entries from {combined_file}:\")\n",
        "\n",
        "try:\n",
        "    with open(combined_file, 'r', encoding='utf-8') as f:\n",
        "        for i in range(5):\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            try:\n",
        "                issue_data = json.loads(line)\n",
        "                print(f\"\\n--- Sample Issue {i+1} ---\")\n",
        "                print(json.dumps(issue_data, indent=4))\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding JSON on line {i+1}. Line content: {line.strip()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {combined_file} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while reading the file: {e}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Displaying sample entries from all_issues_transformed.jsonl:\n",
            "\n",
            "--- Sample Issue 1 ---\n",
            "{\n",
            "    \"key\": \"SPARK-290\",\n",
            "    \"summary\": \"Use SPARK_MASTER_IP if it is set in start-slaves.sh.\",\n",
            "    \"description\": \"Also check to prompt user if it is not set and the script cannot figure out the master's ip.\",\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Reynold Xin\",\n",
            "    \"created\": \"0012-10-19T00:09:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:34.000+0000\",\n",
            "    \"comments\": [],\n",
            "    \"summarization_task\": {\n",
            "        \"instruction\": \"Summarize the following Jira issue:\",\n",
            "        \"input\": \"Summary: Use SPARK_MASTER_IP if it is set in start-slaves.sh.\\nDescription: Also check to prompt user if it is not set and the script cannot figure out the master's ip.\\nComments: \",\n",
            "        \"output\": \"Generated summary goes here.\"\n",
            "    },\n",
            "    \"classification_task\": {\n",
            "        \"instruction\": \"Classify the type of the following Jira issue:\",\n",
            "        \"input\": \"Summary: Use SPARK_MASTER_IP if it is set in start-slaves.sh.\\nDescription: Also check to prompt user if it is not set and the script cannot figure out the master's ip.\",\n",
            "        \"output\": \"Resolved\"\n",
            "    },\n",
            "    \"qna_task\": {\n",
            "        \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
            "        \"input\": \"Issue: Summary: Use SPARK_MASTER_IP if it is set in start-slaves.sh.\\nDescription: Also check to prompt user if it is not set and the script cannot figure out the master's ip.\\nComments: \\nQuestion: What is the main problem described in this issue?\",\n",
            "        \"output\": \"Use SPARK_MASTER_IP if it is set in start-slaves.sh.\"\n",
            "    }\n",
            "}\n",
            "\n",
            "--- Sample Issue 2 ---\n",
            "{\n",
            "    \"key\": \"SPARK-291\",\n",
            "    \"summary\": \"Removing credentials line in build.\",\n",
            "    \"description\": \"This line isn't needed - in fact it messes with the right way of resolving credentials automatically.\",\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Patrick Wendell\",\n",
            "    \"created\": \"0012-10-14T18:35:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:32.000+0000\",\n",
            "    \"comments\": [],\n",
            "    \"summarization_task\": {\n",
            "        \"instruction\": \"Summarize the following Jira issue:\",\n",
            "        \"input\": \"Summary: Removing credentials line in build.\\nDescription: This line isn't needed - in fact it messes with the right way of resolving credentials automatically.\\nComments: \",\n",
            "        \"output\": \"Generated summary goes here.\"\n",
            "    },\n",
            "    \"classification_task\": {\n",
            "        \"instruction\": \"Classify the type of the following Jira issue:\",\n",
            "        \"input\": \"Summary: Removing credentials line in build.\\nDescription: This line isn't needed - in fact it messes with the right way of resolving credentials automatically.\",\n",
            "        \"output\": \"Resolved\"\n",
            "    },\n",
            "    \"qna_task\": {\n",
            "        \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
            "        \"input\": \"Issue: Summary: Removing credentials line in build.\\nDescription: This line isn't needed - in fact it messes with the right way of resolving credentials automatically.\\nComments: \\nQuestion: What is the main problem described in this issue?\",\n",
            "        \"output\": \"Removing credentials line in build.\"\n",
            "    }\n",
            "}\n",
            "\n",
            "--- Sample Issue 3 ---\n",
            "{\n",
            "    \"key\": \"SPARK-292\",\n",
            "    \"summary\": \"Adding dependency repos in quickstart example\",\n",
            "    \"description\": null,\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Patrick Wendell\",\n",
            "    \"created\": \"0012-10-14T10:50:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:30.000+0000\",\n",
            "    \"comments\": [],\n",
            "    \"summarization_task\": {\n",
            "        \"instruction\": \"Summarize the following Jira issue:\",\n",
            "        \"input\": \"Summary: Adding dependency repos in quickstart example\\nDescription: None\\nComments: \",\n",
            "        \"output\": \"Generated summary goes here.\"\n",
            "    },\n",
            "    \"classification_task\": {\n",
            "        \"instruction\": \"Classify the type of the following Jira issue:\",\n",
            "        \"input\": \"Summary: Adding dependency repos in quickstart example\\nDescription: None\",\n",
            "        \"output\": \"Resolved\"\n",
            "    },\n",
            "    \"qna_task\": {\n",
            "        \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
            "        \"input\": \"Issue: Summary: Adding dependency repos in quickstart example\\nDescription: None\\nComments: \\nQuestion: What is the main problem described in this issue?\",\n",
            "        \"output\": \"Adding dependency repos in quickstart example\"\n",
            "    }\n",
            "}\n",
            "\n",
            "--- Sample Issue 4 ---\n",
            "{\n",
            "    \"key\": \"SPARK-293\",\n",
            "    \"summary\": \"SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\",\n",
            "    \"description\": \"The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\\n\\n```scala\\nclass ShuffleDependency[K, V, C](\\n    val shuffleId: Int,\\n    @transient rdd: RDD[(K, V)],\\n    val aggregator: Option[Aggregator[K, V, C]],\\n    val partitioner: Partitioner)\\n  extends Dependency(rdd)\\n```\\n\\n`SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances.\\n\\nIn the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`.  If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.\",\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Josh Rosen\",\n",
            "    \"created\": \"0012-10-14T00:45:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:34.000+0000\",\n",
            "    \"comments\": [],\n",
            "    \"summarization_task\": {\n",
            "        \"instruction\": \"Summarize the following Jira issue:\",\n",
            "        \"input\": \"Summary: SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\\nDescription: The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\\n\\n```scala\\nclass ShuffleDependency[K, V, C](\\n    val shuffleId: Int,\\n    @transient rdd: RDD[(K, V)],\\n    val aggregator: Option[Aggregator[K, V, C]],\\n    val partitioner: Partitioner)\\n  extends Dependency(rdd)\\n```\\n\\n`SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances.\\n\\nIn the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`.  If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.\\nComments: \",\n",
            "        \"output\": \"Generated summary goes here.\"\n",
            "    },\n",
            "    \"classification_task\": {\n",
            "        \"instruction\": \"Classify the type of the following Jira issue:\",\n",
            "        \"input\": \"Summary: SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\\nDescription: The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\\n\\n```scala\\nclass ShuffleDependency[K, V, C](\\n    val shuffleId: Int,\\n    @transient rdd: RDD[(K, V)],\\n    val aggregator: Option[Aggregator[K, V, C]],\\n    val partitioner: Partitioner)\\n  extends Dependency(rdd)\\n```\\n\\n`SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances.\\n\\nIn the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`.  If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.\",\n",
            "        \"output\": \"Resolved\"\n",
            "    },\n",
            "    \"qna_task\": {\n",
            "        \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
            "        \"input\": \"Issue: Summary: SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\\nDescription: The `SparkContext.newShuffleId` method should probably be public, because `ShuffleDependency` is public and its constructor requires a valid `shuffleId`:\\n\\n```scala\\nclass ShuffleDependency[K, V, C](\\n    val shuffleId: Int,\\n    @transient rdd: RDD[(K, V)],\\n    val aggregator: Option[Aggregator[K, V, C]],\\n    val partitioner: Partitioner)\\n  extends Dependency(rdd)\\n```\\n\\n`SparkContext.newShuffleId` is currently `private[spark]`, which prevents me from defining custom RDD subclasses in Shark that create their own `ShuffleDependency` instances.\\n\\nIn the current dev branch, `ShuffleDependency` is always supplied with `SparkContext.newShuffleId`.  If it's not important to be able to set a `ShuffleDependency`'s id to an arbitrary value, then perhaps the `shuffleId` field could be moved from its constructor into the class body, where it could be set to `rdd.context.newShuffleId`.\\nComments: \\nQuestion: What is the main problem described in this issue?\",\n",
            "        \"output\": \"SparkContext.newShuffleId should be public or ShuffleDependency should set its own id\"\n",
            "    }\n",
            "}\n",
            "\n",
            "--- Sample Issue 5 ---\n",
            "{\n",
            "    \"key\": \"SPARK-294\",\n",
            "    \"summary\": \"Disable gpg by default.\",\n",
            "    \"description\": null,\n",
            "    \"status\": \"Resolved\",\n",
            "    \"reporter\": \"Reynold Xin\",\n",
            "    \"created\": \"0012-10-13T22:10:00.000+0000\",\n",
            "    \"updated\": \"2012-10-19T22:50:26.000+0000\",\n",
            "    \"comments\": [],\n",
            "    \"summarization_task\": {\n",
            "        \"instruction\": \"Summarize the following Jira issue:\",\n",
            "        \"input\": \"Summary: Disable gpg by default.\\nDescription: None\\nComments: \",\n",
            "        \"output\": \"Generated summary goes here.\"\n",
            "    },\n",
            "    \"classification_task\": {\n",
            "        \"instruction\": \"Classify the type of the following Jira issue:\",\n",
            "        \"input\": \"Summary: Disable gpg by default.\\nDescription: None\",\n",
            "        \"output\": \"Resolved\"\n",
            "    },\n",
            "    \"qna_task\": {\n",
            "        \"instruction\": \"Answer the following question based on the Jira issue:\",\n",
            "        \"input\": \"Issue: Summary: Disable gpg by default.\\nDescription: None\\nComments: \\nQuestion: What is the main problem described in this issue?\",\n",
            "        \"output\": \"Disable gpg by default.\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}